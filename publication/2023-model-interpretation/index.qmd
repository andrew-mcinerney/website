---
title: "Feedforward Neural Networks as Statistical Models: Improving Interpretability Through Uncertainty Quantification"
author: 
  - name: Andrew McInerney
    orcid: 0000-0002-2348-2159
  - Kevin Burke
date: '2022-11-14'
slug: model-interpretation
categories:
  - Neural networks
  - Model interpretation
description: 'arXiv'
subtitle: 'Hypothesis testing and covariate-effect plots for feedforward neural networks'
abstract: ''
links:
  - icon: file-richtext-fill
    name: Preprint
    url: https://arxiv.org/abs/2311.08139
---

## Abstract

Feedforward neural networks (FNNs) are typically viewed as pure prediction algorithms, and their strong predictive performance has led to their use in many machine-learning applications. However, their flexibility comes with an interpretability trade-off; thus, FNNs have been historically less popular among statisticians. Nevertheless, classical statistical theory, such as significance testing and uncertainty quantification, is still relevant. Supplementing FNNs with methods of statistical inference, and covariate-effect visualisations, can shift the focus away from black-box prediction and make FNNs more akin to traditional statistical models. This can allow for more inferential analysis, and, hence, make FNNs more accessible within the statistical-modelling context.

![plotnn results.](featured.png){fig-alt=" Results of the single- and multiple-parameter Wald tests overlaid on the neural
network architecture for the $\texttt{insurance}$ data for various hidden layer sizes, where $q$ denotes
the number of hidden nodes.
"}


